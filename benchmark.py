import os
import time
import json
import subprocess
import platform
from datetime import datetime
from pathlib import Path
import requests
import psutil
import cpuinfo


class SystemBenchmark:
    def __init__(self):
        self.results = {
            'timestamp': datetime.now().isoformat(),
            'system': {},
            'cpu': {},
            'memory': {},
            'disk': {},
            'ollama': {},
            'models': {},
            'recommendations': {}
        }

    def get_system_info(self):
        """–°–æ–±–∏—Ä–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∏—Å—Ç–µ–º–µ"""
        print("üìä –°–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–∏—Å—Ç–µ–º–µ...")

        self.results['system'] = {
            'os': platform.system(),
            'os_version': platform.version(),
            'processor': platform.processor(),
            'machine': platform.machine(),
            'hostname': platform.node(),
            'python_version': platform.python_version(),
        }

        cpu_info = cpuinfo.get_cpu_info()
        self.results['cpu'] = {
            'brand': cpu_info.get('brand_raw', 'Unknown'),
            'cores': psutil.cpu_count(logical=False),
            'threads': psutil.cpu_count(logical=True),
            'frequency_mhz': psutil.cpu_freq().max if psutil.cpu_freq() else 'Unknown',
            'architecture': cpu_info.get('arch', 'Unknown'),
        }

        memory = psutil.virtual_memory()
        self.results['memory'] = {
            'total_gb': round(memory.total / (1024 ** 3), 2),
            'available_gb': round(memory.available / (1024 ** 3), 2),
            'percent_used': memory.percent,
        }

        disk = psutil.disk_usage('/')
        self.results['disk'] = {
            'total_gb': round(disk.total / (1024 ** 3), 2),
            'free_gb': round(disk.free / (1024 ** 3), 2),
            'percent_used': disk.percent,
        }

        print(f"‚úÖ –°–∏—Å—Ç–µ–º–∞: {self.results['system']['os']}")
        print(f"‚úÖ CPU: {self.results['cpu']['cores']} —è–¥–µ—Ä, {self.results['cpu']['threads']} –ø–æ—Ç–æ–∫–æ–≤")
        print(f"‚úÖ RAM: {self.results['memory']['total_gb']} GB")

    def check_ollama_status(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç—É—Å Ollama"""
        print("\nüîÑ –ü—Ä–æ–≤–µ—Ä–∫–∞ Ollama...")

        try:
            start = time.time()
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            ping_time = time.time() - start

            if response.status_code == 200:
                models = response.json().get('models', [])
                self.results['ollama'] = {
                    'running': True,
                    'ping_ms': round(ping_time * 1000, 2),
                    'models_installed': [m.get('name') for m in models],
                    'models_count': len(models)
                }
                print(f"‚úÖ Ollama –∑–∞–ø—É—â–µ–Ω–∞ (–ø–∏–Ω–≥: {self.results['ollama']['ping_ms']} –º—Å)")
                print(f"üì¶ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(models)}")
                return True
        except:
            self.results['ollama'] = {
                'running': False,
                'error': 'Ollama –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞'
            }
            print("‚ùå Ollama –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞")
            return False

    def benchmark_model(self, model_name, test_prompts):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏"""
        print(f"\nüîÑ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ {model_name}...")

        results = {
            'model': model_name,
            'tests': [],
            'avg_time': 0,
            'avg_tokens_per_sec': 0,
            'memory_usage': []
        }

        total_time = 0
        total_tokens = 0

        for i, prompt in enumerate(test_prompts, 1):
            print(f"  –¢–µ—Å—Ç {i}/{len(test_prompts)}...")

            mem_before = psutil.virtual_memory().used / (1024 ** 3)

            start = time.time()
            try:
                response = requests.post(
                    "http://localhost:11434/api/generate",
                    json={
                        "model": model_name,
                        "prompt": prompt,
                        "stream": False,
                        "options": {
                            "num_predict": 100,
                            "temperature": 0.1
                        }
                    },
                    timeout=120
                )

                elapsed = time.time() - start

                if response.status_code == 200:
                    result = response.json()
                    tokens = result.get('eval_count', 0)
                    tokens_per_sec = tokens / elapsed if elapsed > 0 else 0

                    mem_after = psutil.virtual_memory().used / (1024 ** 3)

                    test_result = {
                        'prompt': prompt[:50] + "...",
                        'time_sec': round(elapsed, 2),
                        'tokens': tokens,
                        'tokens_per_sec': round(tokens_per_sec, 2),
                        'memory_delta_gb': round(mem_after - mem_before, 2)
                    }

                    results['tests'].append(test_result)
                    total_time += elapsed
                    total_tokens += tokens

                    print(f"    ‚úÖ {elapsed:.1f} —Å–µ–∫, {tokens_per_sec:.1f} —Ç–æ–∫–µ–Ω/—Å–µ–∫")

            except Exception as e:
                print(f"    ‚ùå –û—à–∏–±–∫–∞: {e}")
                results['tests'].append({
                    'prompt': prompt[:50],
                    'error': str(e)
                })

        if results['tests']:
            results['avg_time'] = round(total_time / len(results['tests']), 2)
            if total_tokens > 0:
                results['avg_tokens_per_sec'] = round(total_tokens / total_time, 2)

        return results

    def test_parallel_performance(self, model_name):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö"""
        print(f"\nüîÑ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã ({model_name})...")

        results = {
            'parallel_1': 0,
            'parallel_2': 0,
            'parallel_4': 0,
            'recommended': 1
        }

        start = time.time()
        try:
            requests.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": model_name,
                    "prompt": "Say 'test'",
                    "stream": False
                },
                timeout=30
            )
            results['parallel_1'] = time.time() - start
        except:
            results['parallel_1'] = 999

        if self.results['cpu']['threads'] >= 4:
            import threading

            def send_request():
                try:
                    requests.post(
                        "http://localhost:11434/api/generate",
                        json={
                            "model": model_name,
                            "prompt": "Say 'test'",
                            "stream": False
                        },
                        timeout=30
                    )
                except:
                    pass

            start = time.time()
            threads = []
            for _ in range(2):
                t = threading.Thread(target=send_request)
                t.start()
                threads.append(t)

            for t in threads:
                t.join(timeout=35)

            results['parallel_2'] = time.time() - start

        if results['parallel_2'] < results['parallel_1'] * 1.5:
            results['recommended'] = 2
        if results['parallel_4'] < results['parallel_1'] * 2:
            results['recommended'] = 4

        return results

    def get_recommendations(self):
        """–§–æ—Ä–º–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ—Å—Ç–æ–≤"""
        mem_gb = self.results['memory']['total_gb']
        cpu_cores = self.results['cpu']['cores']

        if mem_gb >= 32:
            self.results['recommendations']['model'] = 'llama2:13b'
        elif mem_gb >= 16:
            self.results['recommendations']['model'] = 'codellama:7b'
        elif mem_gb >= 8:
            self.results['recommendations']['model'] = 'tinyllama'
        else:
            self.results['recommendations']['model'] = 'phi3:mini'

        self.results['recommendations']['parallel'] = min(cpu_cores, 4)

        self.results['recommendations']['ollama_settings'] = {
            'OLLAMA_NUM_PARALLEL': str(min(cpu_cores, 4)),
            'OLLAMA_MAX_LOADED_MODELS': '2',
            'OLLAMA_KEEP_ALIVE': '10m',
            'OLLAMA_HOST': '0.0.0.0'
        }

        self.results['recommendations']['crewai_settings'] = {
            'temperature': '0.3',
            'max_tokens': '1000',
            'max_iter': '3',
            'cache': 'True'
        }

    def save_results(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–∞–π–ª"""
        filename = f"benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)

        report = f"""# üìä –ë–µ–Ω—á–º–∞—Ä–∫ —Å–∏—Å—Ç–µ–º—ã
**–î–∞—Ç–∞:** {self.results['timestamp']}

## üñ•Ô∏è –°–∏—Å—Ç–µ–º–∞
- –û–°: {self.results['system']['os']}
- –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä: {self.results['cpu']['brand']}
- –Ø–¥—Ä–∞: {self.results['cpu']['cores']} —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö, {self.results['cpu']['threads']} –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö
- –û–ó–£: {self.results['memory']['total_gb']} GB
- –°–≤–æ–±–æ–¥–Ω–æ: {self.results['memory']['available_gb']} GB

## üöÄ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

### –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: **{self.results['recommendations']['model']}**
–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: **{self.results['recommendations']['parallel']}**

### –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Ollama:
OLLAMA_NUM_PARALLEL={self.results['recommendations']['ollama_settings']['OLLAMA_NUM_PARALLEL']}
OLLAMA_MAX_LOADED_MODELS={self.results['recommendations']['ollama_settings']['OLLAMA_MAX_LOADED_MODELS']}
OLLAMA_KEEP_ALIVE={self.results['recommendations']['ollama_settings']['OLLAMA_KEEP_ALIVE']}
OLLAMA_HOST={self.results['recommendations']['ollama_settings']['OLLAMA_HOST']}

### –ù–∞—Å—Ç—Ä–æ–π–∫–∏ CrewAI:
temperature={self.results['recommendations']['crewai_settings']['temperature']}
max_tokens={self.results['recommendations']['crewai_settings']['max_tokens']}
max_iter={self.results['recommendations']['crewai_settings']['max_iter']}
cache={self.results['recommendations']['crewai_settings']['cache']}

"""

        if self.results.get('models'):
            report += "\n## üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–æ–≤\n\n"
            for model, data in self.results['models'].items():
                if data.get('avg_tokens_per_sec'):
                    report += f"### {model}\n"
                    report += f"- –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {data['avg_time']} —Å–µ–∫\n"
                    report += f"- –°–∫–æ—Ä–æ—Å—Ç—å: {data['avg_tokens_per_sec']} —Ç–æ–∫–µ–Ω/—Å–µ–∫\n\n"

        report_file = filename.replace('.json', '_report.md')
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"\n‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"   üìÑ JSON: {filename}")
        print(f"   üìÑ –û—Ç—á–µ—Ç: {report_file}")

        return filename, report_file


def main():
    print("=" * 60)
    print("üî¨ –ë–ï–ù–ß–ú–ê–†–ö –°–ò–°–¢–ï–ú–´ –î–õ–Ø AI OFFICE")
    print("=" * 60)

    benchmark = SystemBenchmark()

    benchmark.get_system_info()

    if not benchmark.check_ollama_status():
        print("\n‚ùå Ollama –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞. –ó–∞–ø—É—Å—Ç–∏—Ç–µ 'ollama serve' –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
        return

    models_to_test = benchmark.results['ollama']['models_installed']

    if not models_to_test:
        print("\n‚ùå –ù–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É:")
        print("   ollama pull tinyllama")
        print("   ollama pull codellama")
        return

    test_prompts = [
        "Say 'hello'",
        "Write a Python function to calculate factorial",
        "Explain what is AI in one sentence"
    ]

    for model in models_to_test[:3]:
        print(f"\n{'=' * 40}")
        print(f"–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï: {model}")
        print('=' * 40)

        model_results = benchmark.benchmark_model(model, test_prompts)
        benchmark.results['models'][model] = model_results

        if len(models_to_test) == 1:
            parallel_results = benchmark.test_parallel_performance(model)
            benchmark.results['parallel'] = parallel_results

    benchmark.get_recommendations()
    json_file, report_file = benchmark.save_results()

    print("\n" + "=" * 60)
    print("‚úÖ –ë–ï–ù–ß–ú–ê–†–ö –ó–ê–í–ï–†–®–ï–ù")
    print("=" * 60)
    print(f"\nüìä –û—Ç–∫—Ä—ã—Ç—å –æ—Ç—á–µ—Ç: {report_file}")


if __name__ == "__main__":
    try:
        import psutil
        import cpuinfo
    except ImportError:
        print("üì¶ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π...")
        subprocess.run(["pip", "install", "psutil", "py-cpuinfo"])
        import psutil
        import cpuinfo

    main()